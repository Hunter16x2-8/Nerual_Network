
# Neurons, Multilayer Perceptrons, and Deep Neural Networks (To process Handwritten Numbers) from Scratch

This repository contains a comprehensive implementation of artificial neurons, Multilayer Perceptrons (MLPs), and Deep Neural Networks (DNNs) built from scratch using Python. The aim is to provide an in-depth understanding of the fundamental building blocks of neural networks and how they come together to solve complex machine learning tasks.

## Key Features
- **Simple Neurons:** Basic implementation of Simple neurons, including forward propagation.
- **Multilayer Perceptron (MLP):** Construction of MLPs with multiple hidden layers, showcasing the power of deep learning in handling non-linear problems.
- **Deep Neural Network (DNN):** Extension of MLP with deeper architectures, capable of learning hierarchical representations of data. In this repository the DNN made was very Simple just to understand Basic and DNN in this repository contains only 2 hidden layers.
- **Activation Functions:** Implementation of various activation functions such as Softmax, and ReLU.
- **Loss Functions:** Loss Functions used for Training is Cross-Entropy Loss 
- **Training Algorithms:** Gradient Descent and its variants (e.g., Stochastic Gradient Descent) for optimizing the network parameters.



## Deployment

To run this Repository Code 

1. Clone the repository:
   ```bash
   git clone 
    ```
2. Navigate to Project directory:
    ```bash
    cd Nerual_Network
    ```
3. Create Virtual enviroment and activate it.

4. Install the required dependencies:
    ```bash
    pip install -r requirements.txt
    ```
5. To run Jupyter Notebook code or to run Nerual_Network.py file code
- Enter the csv file location in pd.read_csv("here")
- Make sure to add seprated csv because one is for Training and other is for Test